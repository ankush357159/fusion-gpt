{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcO-GkvOsarL",
        "outputId": "016b811e-cb7a-41dc-9073-b77aba742e7a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'fusion-gpt'...\n",
            "remote: Enumerating objects: 278, done.\u001b[K\n",
            "remote: Counting objects: 100% (278/278), done.\u001b[K\n",
            "remote: Compressing objects: 100% (196/196), done.\u001b[K\n",
            "remote: Total 278 (delta 124), reused 215 (delta 63), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (278/278), 67.56 KiB | 1.09 MiB/s, done.\n",
            "Resolving deltas: 100% (124/124), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ankush357159/fusion-gpt.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/fusion-gpt/picogpt\n",
        "!git pull"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FzgK9Gum9Gb3",
        "outputId": "78568e32-cf4b-41ef-9124-f55b616d253a"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/fusion-gpt/picogpt\n",
            "remote: Enumerating objects: 13, done.\u001b[K\n",
            "remote: Counting objects: 100% (13/13), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1/1), done.\u001b[K\n",
            "remote: Total 7 (delta 6), reused 7 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Unpacking objects: 100% (7/7), 623 bytes | 311.00 KiB/s, done.\n",
            "From https://github.com/ankush357159/fusion-gpt\n",
            "   27f1179..6ddcadc  main       -> origin/main\n",
            "Updating 27f1179..6ddcadc\n",
            "Fast-forward\n",
            " picogpt/data/raw/data.py    | 14 \u001b[32m+++++++\u001b[m\u001b[31m-------\u001b[m\n",
            " picogpt/test_generate.ipynb | 11 \u001b[32m+++++++++++\u001b[m\n",
            " 2 files changed, 18 insertions(+), 7 deletions(-)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/fusion-gpt/picogpt\")"
      ],
      "metadata": {
        "id": "WY5M3uEm9mk2"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from data.raw.data import download_books, combine_books\n",
        "\n",
        "# 1. Download the raw .txt files from Project Gutenberg\n",
        "download_books()\n",
        "\n",
        "# 2. Process, clean markers, and merge into 'combined_novels.txt'\n",
        "input_books = combine_books()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cgt2TEhu9q_m",
        "outputId": "0066a3ea-fdb5-49b8-a810-3c291fe68f88"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading books...\n",
            "- Skipping gatsby (already downloaded)\n",
            "Download complete\n",
            "\n",
            "Cleaning and combining texts...\n",
            "Dataset ready!\n",
            "Saved to: /content/fusion-gpt/picogpt/data/raw/combined_novels.txt\n",
            "Total characters: 271,227\n",
            "Approx size: 0.27 MB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pathlib import Path\n",
        "\n",
        "combined_path = Path(\"/content/fusion-gpt/picogpt/data/raw/combined_novels.txt\")\n",
        "\n",
        "input_books = combined_path.read_text(encoding=\"utf-8\")\n",
        "\n",
        "print(\"Loaded characters:\", len(input_books))\n",
        "print(input_books[:500])  # preview\n"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LouCBRaE9x5F",
        "outputId": "2ae25ca9-e250-4720-d9c2-3d0bf586c058"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded characters: 271227\n",
            "The Great Gatsby\n",
            "                                  by\n",
            "                          F. Scott Fitzgerald\n",
            "\n",
            "\n",
            "                           Table of Contents\n",
            "\n",
            "I\n",
            "II\n",
            "III\n",
            "IV\n",
            "V\n",
            "VI\n",
            "VII\n",
            "VIII\n",
            "IX\n",
            "\n",
            "\n",
            "                              Once again\n",
            "                                  to\n",
            "                                 Zelda\n",
            "\n",
            "\n",
            "  Then wear the gold hat, if that will move her;\n",
            "  If you can bounce high, bounce for her too,\n",
            "  Till she cry “Lover, gold-hatted, high-bouncing lover,\n",
            "  I must have you!”\n",
            "\n",
            "  Thomas Parke d’Invilliers\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize + split into train/val/test\n",
        "!python -m src.training.prepare_splits \\\n",
        "  --input_text /content/fusion-gpt/picogpt/data/raw/combined_novels.txt \\\n",
        "  --train_split 0.9 --val_split 0.05 --test_split 0.05 \\\n",
        "  --output_dir /content/fusion-gpt/picogpt/data/processed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tnliJbRx96au",
        "outputId": "f876eb81-6e04-463a-cb8e-cb42cd579b8a"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split complete:\n",
            "- Train tokens: /content/fusion-gpt/picogpt/data/processed/train_tokens.npy (70,217)\n",
            "- Val tokens:   /content/fusion-gpt/picogpt/data/processed/val_tokens.npy (3,900)\n",
            "- Test tokens:  /content/fusion-gpt/picogpt/data/processed/test_tokens.npy (3,902)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "train_tokens_path = '/content/fusion-gpt/picogpt/data/processed/train_tokens.npy'\n",
        "\n",
        "# Check if the file exists\n",
        "if os.path.exists(train_tokens_path):\n",
        "    print(f\"File '{train_tokens_path}' exists.\")\n",
        "    # Check file size\n",
        "    file_size = os.path.getsize(train_tokens_path)\n",
        "    print(f\"File size: {file_size} bytes\")\n",
        "\n",
        "    # Attempt to load the file\n",
        "    try:\n",
        "        loaded_tokens = np.load(train_tokens_path)\n",
        "        print(f\"Successfully loaded '{train_tokens_path}'.\")\n",
        "        print(f\"Shape of loaded tokens: {loaded_tokens.shape}\")\n",
        "        print(f\"First 10 tokens: {loaded_tokens[:10]}\")\n",
        "    except EOFError:\n",
        "        print(f\"EOFError: '{train_tokens_path}' appears to be empty or corrupted.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while loading '{train_tokens_path}': {e}\")\n",
        "else:\n",
        "    print(f\"File '{train_tokens_path}' does not exist. This is unexpected.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GZ7Ym8Gd-NZO",
        "outputId": "d7a2962c-86c9-4b68-c302-af92fd43f358"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File '/content/fusion-gpt/picogpt/data/processed/train_tokens.npy' exists.\n",
            "File size: 561864 bytes\n",
            "Successfully loaded '/content/fusion-gpt/picogpt/data/processed/train_tokens.npy'.\n",
            "Shape of loaded tokens: (70217,)\n",
            "First 10 tokens: [ 464 3878  402 1381 1525  198  220  220  220  220]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train (uses pre-tokenized splits via train_tokens.npy)\n",
        "!python -m src.training.train \\\n",
        "  --tokens_path /content/fusion-gpt/picogpt/data/processed/train_tokens.npy \\\n",
        "  --block_size 128 --batch_size 32 --epochs 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbze7J3n-clW",
        "outputId": "4357e7cf-9643-41ae-8d30-105724c3526e"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training PicoGPT from scratch\n",
            "Total tokens: 70,217\n",
            "Train tokens: 63,195\n",
            "Val tokens:   3,511\n",
            "Test tokens:  3,511\n",
            "Epoch 1/1 [train]: 100% 1970/1970 [05:46<00:00,  5.68it/s, loss=2.8097]\n",
            "Epoch 1/1 [val]: 100% 105/105 [00:06<00:00, 16.49it/s, loss=5.2361]\n",
            "Epoch 1 | Train Loss: 3.6761 | Val Loss: 5.2074\n",
            "Saved new best checkpoint.\n",
            "Scratch training complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "HXKwkGlcG6qv",
        "outputId": "c78fa5e8-1ba4-4c1d-8b16-5d1ea2ba5dbd"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cuda'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from configs.GPTConfig import GPTConfig\n",
        "from src.tokenizer.tokenizer import TiktokenTokenizer\n",
        "\n",
        "tokenizer = TiktokenTokenizer(encoding_name=\"gpt2\")\n",
        "config = GPTConfig(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    block_size=128,\n",
        "    embed_dim=256,\n",
        "    num_heads=8,\n",
        "    num_layers=6,\n",
        "    use_rope=False,\n",
        "    gpt2_compatible=False,\n",
        ")\n",
        "config"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5VRJbF9G-om",
        "outputId": "ab3601a4-8f1f-4c8e-d445-e214c362b23f"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<configs.GPTConfig.GPTConfig at 0x7ae59d72f6b0>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.model.pico_gpt import PicoGPT\n",
        "\n",
        "model = PicoGPT(config).to(device)\n",
        "model.block_size = config.block_size\n",
        "\n",
        "checkpoint_path = \"checkpoints/picogpt_best.pt\"\n",
        "state_dict = torch.load(checkpoint_path, map_location=device)\n",
        "model.load_state_dict(state_dict)\n",
        "model.eval()\n",
        "\"Checkpoint loaded\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "mXh37O7UHBuW",
        "outputId": "d847418d-34cb-4dea-c39c-fd565852d312"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Checkpoint loaded'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from src.inference.generate import generate\n",
        "\n",
        "prompt = \"Please generate short poem\"\n",
        "output = generate(model, tokenizer, prompt, max_new_tokens=100, device=device)\n",
        "print(output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tMKtxaf0HEFu",
        "outputId": "08bcf66e-a179-47e8-af94-1a4b13b8d142"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Please generate short poem,\n",
            "and you oughtnervous: “I killed a girl so it from get half\n",
            "had care what kind of him—”\n",
            "\n",
            "“I’m talk to ask too.”\n",
            "\n",
            "“You know.”\n",
            "\n",
            "“You’re a great people of interesting fellow’s been going to endure within Tom.”\n",
            "\n",
            "“Was to get some time.”\n",
            "\n",
            "“Wake me.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a4d57bbf",
        "outputId": "5c2f15f3-a897-421f-fb7b-1a0b0445fb49"
      },
      "source": [
        "import torch\n",
        "\n",
        "checkpoint_path = \"checkpoints/picogpt_best.pt\"\n",
        "state_dict = torch.load(checkpoint_path, map_location='cpu') # Load to CPU to avoid CUDA issues if not available\n",
        "\n",
        "print(\"Keys in the state_dict:\")\n",
        "for key in state_dict.keys():\n",
        "    print(key)\n",
        "\n",
        "print(\"\\nSample weights (first parameter found):\")\n",
        "# Get the first key and its corresponding tensor\n",
        "first_param_key = next(iter(state_dict))\n",
        "first_param_tensor = state_dict[first_param_key]\n",
        "\n",
        "print(f\"Parameter name: {first_param_key}\")\n",
        "print(f\"Shape: {first_param_tensor.shape}\")\n",
        "print(f\"Sample values (first 10 elements):\\n{first_param_tensor.flatten()[:10]}\\n\")\n",
        "\n",
        "# Optionally, you can print sample weights from another specific layer, e.g., 'transformer.h.0.attn.c_attn.weight'\n",
        "if 'transformer.h.0.attn.c_attn.weight' in state_dict:\n",
        "    attn_weight = state_dict['transformer.h.0.attn.c_attn.weight']\n",
        "    print(f\"Parameter name: transformer.h.0.attn.c_attn.weight\")\n",
        "    print(f\"Shape: {attn_weight.shape}\")\n",
        "    print(f\"Sample values (first 10 elements):\\n{attn_weight.flatten()[:10]}\")"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keys in the state_dict:\n",
            "embeddings.token_emb.weight\n",
            "embeddings.pos_emb.weight\n",
            "blocks.0.ln1.weight\n",
            "blocks.0.ln1.bias\n",
            "blocks.0.ln2.weight\n",
            "blocks.0.ln2.bias\n",
            "blocks.0.attn.mask\n",
            "blocks.0.attn.qkv.weight\n",
            "blocks.0.attn.qkv.bias\n",
            "blocks.0.attn.proj.weight\n",
            "blocks.0.attn.proj.bias\n",
            "blocks.0.ff.net.0.weight\n",
            "blocks.0.ff.net.0.bias\n",
            "blocks.0.ff.net.2.weight\n",
            "blocks.0.ff.net.2.bias\n",
            "blocks.1.ln1.weight\n",
            "blocks.1.ln1.bias\n",
            "blocks.1.ln2.weight\n",
            "blocks.1.ln2.bias\n",
            "blocks.1.attn.mask\n",
            "blocks.1.attn.qkv.weight\n",
            "blocks.1.attn.qkv.bias\n",
            "blocks.1.attn.proj.weight\n",
            "blocks.1.attn.proj.bias\n",
            "blocks.1.ff.net.0.weight\n",
            "blocks.1.ff.net.0.bias\n",
            "blocks.1.ff.net.2.weight\n",
            "blocks.1.ff.net.2.bias\n",
            "blocks.2.ln1.weight\n",
            "blocks.2.ln1.bias\n",
            "blocks.2.ln2.weight\n",
            "blocks.2.ln2.bias\n",
            "blocks.2.attn.mask\n",
            "blocks.2.attn.qkv.weight\n",
            "blocks.2.attn.qkv.bias\n",
            "blocks.2.attn.proj.weight\n",
            "blocks.2.attn.proj.bias\n",
            "blocks.2.ff.net.0.weight\n",
            "blocks.2.ff.net.0.bias\n",
            "blocks.2.ff.net.2.weight\n",
            "blocks.2.ff.net.2.bias\n",
            "blocks.3.ln1.weight\n",
            "blocks.3.ln1.bias\n",
            "blocks.3.ln2.weight\n",
            "blocks.3.ln2.bias\n",
            "blocks.3.attn.mask\n",
            "blocks.3.attn.qkv.weight\n",
            "blocks.3.attn.qkv.bias\n",
            "blocks.3.attn.proj.weight\n",
            "blocks.3.attn.proj.bias\n",
            "blocks.3.ff.net.0.weight\n",
            "blocks.3.ff.net.0.bias\n",
            "blocks.3.ff.net.2.weight\n",
            "blocks.3.ff.net.2.bias\n",
            "blocks.4.ln1.weight\n",
            "blocks.4.ln1.bias\n",
            "blocks.4.ln2.weight\n",
            "blocks.4.ln2.bias\n",
            "blocks.4.attn.mask\n",
            "blocks.4.attn.qkv.weight\n",
            "blocks.4.attn.qkv.bias\n",
            "blocks.4.attn.proj.weight\n",
            "blocks.4.attn.proj.bias\n",
            "blocks.4.ff.net.0.weight\n",
            "blocks.4.ff.net.0.bias\n",
            "blocks.4.ff.net.2.weight\n",
            "blocks.4.ff.net.2.bias\n",
            "blocks.5.ln1.weight\n",
            "blocks.5.ln1.bias\n",
            "blocks.5.ln2.weight\n",
            "blocks.5.ln2.bias\n",
            "blocks.5.attn.mask\n",
            "blocks.5.attn.qkv.weight\n",
            "blocks.5.attn.qkv.bias\n",
            "blocks.5.attn.proj.weight\n",
            "blocks.5.attn.proj.bias\n",
            "blocks.5.ff.net.0.weight\n",
            "blocks.5.ff.net.0.bias\n",
            "blocks.5.ff.net.2.weight\n",
            "blocks.5.ff.net.2.bias\n",
            "ln_f.weight\n",
            "ln_f.bias\n",
            "head.weight\n",
            "\n",
            "Sample weights (first parameter found):\n",
            "Parameter name: embeddings.token_emb.weight\n",
            "Shape: torch.Size([50257, 256])\n",
            "Sample values (first 10 elements):\n",
            "tensor([ 0.7190, -0.9309, -1.1073,  0.6533, -0.0778,  2.0270,  0.4787,  1.5795,\n",
            "         0.0984, -0.3475])\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ac1ee98"
      },
      "source": [
        "This code loads the `state_dict` from the checkpoint file. It then prints all the keys (names of the parameters) available in the state dictionary. Finally, it demonstrates how to access and print the shape and a few sample values from the first parameter in the dictionary, and also for a specific attention layer weight if it exists."
      ]
    }
  ]
}