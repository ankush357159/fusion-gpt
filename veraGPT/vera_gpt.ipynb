{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e85eb36",
   "metadata": {},
   "source": [
    "# VeraGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d212551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set your GitHub repo URL ---\n",
    "REPO_URL = \"https://github.com/ankush357159/fusion-gpt.git\"\n",
    "REPO_DIR = \"/content/fusion-gpt\"\n",
    "\n",
    "# Clone (or re-clone) the repo\n",
    "import os\n",
    "\n",
    "# Ensure we are in a stable directory before attempting to remove and clone\n",
    "%cd /content\n",
    "\n",
    "if os.path.isdir(REPO_DIR):\n",
    "    !rm -rf \"$REPO_DIR\"\n",
    "!git clone \"$REPO_URL\" \"$REPO_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install veraGPT dependencies\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "!pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b86d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) If your model is gated/private, set your HF token\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"  # <- paste token or leave blank for public models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84d415",
   "metadata": {},
   "source": [
    "### Step 1: Choose Your Model Preset\n",
    "\n",
    "Pick a model based on your hardware and speed/quality needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect hardware and show available presets\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "import sys\n",
    "sys.path.insert(0, '/content/fusion-gpt/veraGPT/src')\n",
    "\n",
    "import torch\n",
    "from model_presets import PRESETS, recommend_preset\n",
    "\n",
    "# Detect hardware\n",
    "has_gpu = torch.cuda.is_available()\n",
    "vram_gb = 16 if has_gpu else 0  # T4 has 16GB\n",
    "\n",
    "print(\"HARDWARE DETECTED\")\n",
    "print(\"=\"*60)\n",
    "if has_gpu:\n",
    "    print(\"GPU: T4 GPU available (16 GB VRAM)\")\n",
    "    print(\"You can use any model below!\")\n",
    "else:\n",
    "    print(\"CPU Only: No GPU detected\")\n",
    "    print(\"Only 'tiny' preset recommended (others will fail with OOM)\")\n",
    "    print(\"\\nTo enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
    "\n",
    "# Show available presets\n",
    "print(\"\\nAVAILABLE MODEL PRESETS\")\n",
    "print(\"=\"*60)\n",
    "for key, preset in PRESETS.items():\n",
    "    icon = \"YES\" if (has_gpu or key == \"tiny\") else \"NO\"\n",
    "    print(f\"\\n{icon} '{key}': {preset.name}\")\n",
    "    print(f\"   Model: {preset.model_id}\")\n",
    "    print(f\"   Quality: {preset.quality} | GPU Speed: {preset.speed_gpu}\")\n",
    "    if key == \"tiny\":\n",
    "        print(f\"Works on: CPU + GPU (fastest option)\")\n",
    "    else:\n",
    "        print(f\"Requires: T4 GPU (min {preset.min_vram_gb}GB VRAM)\")\n",
    "\n",
    "# Auto-recommend\n",
    "recommended = recommend_preset(has_gpu, vram_gb)\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"AUTO-RECOMMENDED: '{recommended}' ({PRESETS[recommended].name})\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Set default (user can override below)\n",
    "SELECTED_PRESET = recommended\n",
    "print(f\"\\nCurrent selection: '{SELECTED_PRESET}'\")\n",
    "print(\"To change: Edit SELECTED_PRESET in the cell below\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a58ea637",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHANGE THIS to override auto-selection\n",
    "# Options: 'tiny', 'phi2', 'phi3', 'mistral', 'llama13'\n",
    "\n",
    "SELECTED_PRESET = \"tiny\"  # ← Change this!\n",
    "\n",
    "# Validate\n",
    "from model_presets import PRESETS\n",
    "if SELECTED_PRESET not in PRESETS:\n",
    "    print(f\"Invalid preset: '{SELECTED_PRESET}'\")\n",
    "    print(f\"Valid options: {list(PRESETS.keys())}\")\n",
    "else:\n",
    "    preset = PRESETS[SELECTED_PRESET]\n",
    "    print(f\"Selected: '{SELECTED_PRESET}' ({preset.name})\")\n",
    "    print(f\"   Model: {preset.model_id}\")\n",
    "    print(f\"   Quality: {preset.quality} | Speed: {preset.speed_gpu}\")\n",
    "    \n",
    "    # Hardware check\n",
    "    import torch\n",
    "    if not torch.cuda.is_available() and SELECTED_PRESET != \"tiny\":\n",
    "        print(f\"\\nWARNING: '{SELECTED_PRESET}' requires GPU but CPU detected!\")\n",
    "        print(\"This will likely fail with OOM or be extremely slow.\")\n",
    "        print(\"Recommended: SELECTED_PRESET = 'tiny'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3daac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Load Model with Selected Preset\n",
    "\n",
    "# This will load the model once and keep it in memory for fast responses (30-60 seconds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89323962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model using selected preset\n",
    "from config import Config\n",
    "from device_manager import DeviceManager\n",
    "from model_loader import ModelLoader\n",
    "from inference_engine import InferenceEngine\n",
    "from server import ModelServer\n",
    "\n",
    "print(f\"Loading model with preset: '{SELECTED_PRESET}'...\")\n",
    "\n",
    "# Create config from preset\n",
    "# enable_quantization=True for GPU (4-bit), False for CPU\n",
    "import torch\n",
    "enable_quant = torch.cuda.is_available()  # Only quantize on GPU\n",
    "\n",
    "config = Config.from_preset(SELECTED_PRESET, enable_quantization=enable_quant)\n",
    "print(f\"Model ID: {config.model.model_name_or_path}\")\n",
    "print(f\"Quantization: {'Enabled (4-bit)' if config.quantization.enabled else 'Disabled'}\")\n",
    "\n",
    "# Initialize server (loads model once)\n",
    "server = ModelServer(config)\n",
    "print(\"\\nLoading model (this may take 30-60 seconds)...\")\n",
    "server.load()\n",
    "print(\"Model loaded and ready!\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad039c6",
   "metadata": {},
   "source": [
    "### Step 3: Chat with the Model\n",
    "\n",
    "Try asking questions! The model is already loaded, so each response will be fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8158f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask your first question\n",
    "prompt = \"Explain what is Python programming language in 2 sentences.\"\n",
    "\n",
    "print(f\"Question: {prompt}\\n\")\n",
    "response = server.ask(prompt, show_timing=True)\n",
    "print(f\"\\nAnswer: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question (FAST - no model reloading!)\n",
    "response = server.ask(\n",
    "    \"Please explain Newton's second law of motion\",\n",
    "    show_timing=True\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask another question (still FAST!)\n",
    "response = server.ask(\n",
    "    \"What is quantum entanglement?\",\n",
    "    show_timing=True\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c577897",
   "metadata": {},
   "source": [
    "### Optional: Switch to a Different Model\n",
    "\n",
    "Want to try another model? Change the preset and reload."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c79ea59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change preset and reload\n",
    "SELECTED_PRESET = \"phi2\"  # Try: 'phi2', 'phi3', 'mistral', 'llama13'\n",
    "\n",
    "# Reload with new preset\n",
    "config = Config.from_preset(SELECTED_PRESET, enable_quantization=torch.cuda.is_available())\n",
    "server = ModelServer(config)\n",
    "print(f\"Switching to {SELECTED_PRESET}...\")\n",
    "server.load()\n",
    "print(\"Model switched!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea46e8",
   "metadata": {},
   "source": [
    "### Troubleshooting Guide\n",
    "\n",
    "### Issue: Process Killed / OOM (Out of Memory)\n",
    "\n",
    "**Cause**: Model requires more RAM than available\n",
    "\n",
    "**Solution Options**:\n",
    "\n",
    "**Option 1: Switch to Smaller Model (Recommended)**\n",
    "\n",
    "```python\n",
    "# In cell 7, change to:\n",
    "SELECTED_PRESET = \"tiny\"  # TinyLlama works on CPU with only 2-3GB RAM\n",
    "```\n",
    "\n",
    "**Option 2: Enable GPU**\n",
    "\n",
    "```\n",
    "1. Runtime → Change runtime type → T4 GPU\n",
    "2. Re-run all cells\n",
    "3. Any preset will work on T4 GPU\n",
    "```\n",
    "\n",
    "### Model Requirements:\n",
    "\n",
    "| Preset    | Model           | RAM Needed | CPU Support | GPU Speed   |\n",
    "| --------- | --------------- | ---------- | ----------- | ----------- |\n",
    "| `tiny`    | TinyLlama 1.1B  | 2-3 GB     | Yes         | (15 tok/s)  |\n",
    "| `phi2`    | Microsoft Phi-2 | 6-8 GB     | Tight       | (12 tok/s)  |\n",
    "| `phi3`    | Microsoft Phi-3 | 8-10 GB    | No          | (10 tok/s)  |\n",
    "| `mistral` | Mistral 7B      | 14-18 GB   | No          | (3 tok/s)   |\n",
    "| `llama13` | Llama-2 13B     | 26-32 GB   | No          | (1-2 tok/s) |\n",
    "\n",
    "### Hardware Available:\n",
    "\n",
    "- **Colab CPU**: 12 GB RAM → Use `tiny` only\n",
    "- **Colab T4 GPU**: 16 GB VRAM → Use `tiny`, `phi2`, `phi3`, `mistral`\n",
    "- **Colab A100 GPU**: 40 GB VRAM → Any preset works\n",
    "\n",
    "### Speed vs Quality Trade-off:\n",
    "\n",
    "- **Fastest**: `tiny` (TinyLlama) - Good for simple tasks, casual chat\n",
    "- **Balanced**: `phi2`, `phi3` - Better reasoning, still fast\n",
    "- **Best Quality**: `mistral`, `llama13` - Complex tasks, slower\n",
    "\n",
    "### How to Check Current Hardware:\n",
    "\n",
    "```python\n",
    "import torch\n",
    "print(f\"GPU: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
