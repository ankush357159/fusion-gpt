{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e85eb36",
   "metadata": {},
   "source": [
    "# VeraGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d212551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set your GitHub repo URL ---\n",
    "REPO_URL = \"https://github.com/ankush357159/fusion-gpt.git\"\n",
    "REPO_DIR = \"/content/fusion-gpt\"\n",
    "\n",
    "# Clone (or re-clone) the repo\n",
    "import os\n",
    "\n",
    "# Ensure we are in a stable directory before attempting to remove and clone\n",
    "%cd /content\n",
    "\n",
    "if os.path.isdir(REPO_DIR):\n",
    "    !rm -rf \"$REPO_DIR\"\n",
    "!git clone \"$REPO_URL\" \"$REPO_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install veraGPT dependencies\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "!pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b86d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) If your model is gated/private, set your HF token\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"  # <- paste token or leave blank for public models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3daac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single prompt (non-interactive)\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "!python src/main.py --prompt \"Write a short welcome message for veraGPT.\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad039c6",
   "metadata": {},
   "source": [
    "## OPTION 2: Persistent Model Server (RECOMMENDED for Colab)\n",
    "\n",
    "**Load model once, then ask multiple questions without reloading.**  \n",
    "This is **10-100x faster** for subsequent prompts since the model stays in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8158f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model ONCE (takes ~60s on T4 GPU)\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "import sys\n",
    "sys.path.insert(0, '/content/fusion-gpt/veraGPT/src')\n",
    "\n",
    "from server import ModelServer\n",
    "\n",
    "# Initialize and load model\n",
    "server = ModelServer()\n",
    "server.load()  # This takes time - but only run once!\n",
    "\n",
    "print(\"\\nâœ… Model loaded! Now you can ask questions quickly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question (FAST - no model reloading!)\n",
    "response = server.ask(\n",
    "    \"Please explain Newton's second law of motion\",\n",
    "    show_timing=True\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask another question (still FAST!)\n",
    "response = server.ask(\n",
    "    \"What is quantum entanglement?\",\n",
    "    show_timing=True\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c577897",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- For quantized loading, add `--quant 4` or `--quant 8` (CUDA only).\n",
    "- To load a LoRA adapter, add `--lora-path /path/to/adapter`.\n",
    "- Interactive mode is not ideal in Colab; prefer the single-prompt cell."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
