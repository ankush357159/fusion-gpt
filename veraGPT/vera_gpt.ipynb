{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e85eb36",
   "metadata": {},
   "source": [
    "# VeraGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d212551",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Set your GitHub repo URL ---\n",
    "REPO_URL = \"https://github.com/ankush357159/fusion-gpt.git\"\n",
    "REPO_DIR = \"/content/fusion-gpt\"\n",
    "\n",
    "# Clone (or re-clone) the repo\n",
    "import os\n",
    "\n",
    "# Ensure we are in a stable directory before attempting to remove and clone\n",
    "%cd /content\n",
    "\n",
    "if os.path.isdir(REPO_DIR):\n",
    "    !rm -rf \"$REPO_DIR\"\n",
    "!git clone \"$REPO_URL\" \"$REPO_DIR\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a9b7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install veraGPT dependencies\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "!pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94b86d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) If your model is gated/private, set your HF token\n",
    "import os\n",
    "os.environ[\"HUGGINGFACE_HUB_TOKEN\"] = \"\"  # <- paste token or leave blank for public models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a84d415",
   "metadata": {},
   "source": [
    "### Check Runtime\n",
    "\n",
    "**Colab CPU**: Use TinyLlama (smaller model)  \n",
    "**Colab T4 GPU**: Can use any model (Mistral-7B recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9762772c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect runtime and recommend model\n",
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU Available: T4 GPU detected\")\n",
    "    print(\"Recommended: Use any model (Mistral-7B, Phi-2, or TinyLlama)\")\n",
    "    RECOMMENDED_MODEL = \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
    "else:\n",
    "    print(\"CPU Only: GPU not available\")\n",
    "    print(\"WARNING: Large models (Mistral-7B) will fail with OOM on CPU!\")\n",
    "    print(\"Recommended: Use TinyLlama-1.1B (only 2GB RAM)\")\n",
    "    print(\"\\nTo enable GPU: Runtime → Change runtime type → T4 GPU\")\n",
    "    RECOMMENDED_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "print(f\"\\nRecommended model: {RECOMMENDED_MODEL}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3daac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a single prompt with auto-detected model\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "\n",
    "# Use recommended model based on runtime\n",
    "!python src/main.py --model \"$RECOMMENDED_MODEL\" --prompt \"Write a short welcome message for veraGPT.\" --timing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad039c6",
   "metadata": {},
   "source": [
    "### OPTION 2: Persistent Model Server (RECOMMENDED for Colab)\n",
    "\n",
    "**Load model once, then ask multiple questions without reloading.**  \n",
    "This is **10-100x faster** for subsequent prompts since the model stays in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8158f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the model ONCE (uses recommended model based on your runtime)\n",
    "%cd /content/fusion-gpt/veraGPT\n",
    "import sys\n",
    "sys.path.insert(0, '/content/fusion-gpt/veraGPT/src')\n",
    "\n",
    "from server import ModelServer\n",
    "from config import Config\n",
    "\n",
    "# Build config with recommended model\n",
    "cfg = Config.from_env()\n",
    "cfg.model.model_name_or_path = RECOMMENDED_MODEL\n",
    "\n",
    "# Initialize and load model\n",
    "server = ModelServer(cfg)\n",
    "server.load()  # Takes ~15s for TinyLlama, ~60s for Mistral-7B\n",
    "\n",
    "print(f\"\\nModel '{RECOMMENDED_MODEL}' loaded! Now you can ask questions quickly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac2a889",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask a question (FAST - no model reloading!)\n",
    "response = server.ask(\n",
    "    \"Please explain Newton's second law of motion\",\n",
    "    show_timing=True\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad5d046",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ask another question (still FAST!)\n",
    "response = server.ask(\n",
    "    \"What is quantum entanglement?\",\n",
    "    show_timing=True\n",
    ")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c577897",
   "metadata": {},
   "source": [
    "## Notes\n",
    "- For quantized loading, add `--quant 4` or `--quant 8` (CUDA only).\n",
    "- To load a LoRA adapter, add `--lora-path /path/to/adapter`.\n",
    "- Interactive mode is not ideal in Colab; prefer the single-prompt cell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ea46e8",
   "metadata": {},
   "source": [
    "### Troubleshooting: OOM Errors on CPU\n",
    "\n",
    "### Problem: Process killed during model loading\n",
    "```\n",
    "Loading checkpoint shards:  33% 1/3 [00:24<00:49, 24.59s/it]^C\n",
    "```\n",
    "\n",
    "### Cause:\n",
    "- **Mistral-7B needs ~14-18 GB RAM**\n",
    "- **Colab CPU only has ~12 GB RAM**\n",
    "- Process gets killed (Out of Memory)\n",
    "\n",
    "### Solutions:\n",
    "\n",
    "**Option 1: Switch to GPU (Best)**\n",
    "```\n",
    "1. Runtime → Change runtime type → T4 GPU\n",
    "2. Re-run cells\n",
    "3. Will work perfectly with Mistral-7B\n",
    "```\n",
    "\n",
    "**Option 2: Use Smaller Model on CPU**\n",
    "```python\n",
    "# Already configured! Cell 5 auto-detects and uses TinyLlama on CPU\n",
    "# Just run cells 5-8 normally\n",
    "```\n",
    "\n",
    "**Option 3: Manual Override**\n",
    "```python\n",
    "# Force TinyLlama even on GPU (for faster responses)\n",
    "RECOMMENDED_MODEL = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "```\n",
    "\n",
    "### Memory Requirements:\n",
    "\n",
    "| Model | RAM Needed | Colab CPU | Colab T4 GPU |\n",
    "|-------|-----------|-----------|--------------|\n",
    "| TinyLlama-1.1B | 2-3 GB | ✅ Works | ✅ Fast (15 tok/s) |\n",
    "| Phi-2 | 6-8 GB | ⚠️ Tight | ✅ Works (12 tok/s) |\n",
    "| Mistral-7B | 14-18 GB | ❌ OOM | ✅ Works (3 tok/s) |\n",
    "\n",
    "### Speed Comparison:\n",
    "\n",
    "| Model | CPU | T4 GPU |\n",
    "|-------|-----|--------|\n",
    "| TinyLlama | 2-5 min/response | 5-10s |\n",
    "| Mistral-7B | ❌ Crashes | 30-60s |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
