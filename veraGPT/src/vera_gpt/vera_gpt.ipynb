{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "72e56602",
   "metadata": {},
   "source": [
    "# VeraGPT (Mistral-Core) Test\n",
    "Run this notebook in Google Colab to load the Mistral-7B-Instruct model and test prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66296d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone repo (skip if already cloned in Colab)\n",
    "import os\n",
    "if not os.path.exists(\"/content/fusion-gpt\"):\n",
    "    !git clone https://github.com/ankush357159/fusion-gpt.git\n",
    "%cd /content/fusion-gpt/veraGPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8505235d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (Colab)\n",
    "!pip -q install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55b23e11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: set your Hugging Face token (needed for gated models)\n",
    "import os\n",
    "os.environ[\"HF_TOKEN\"] = \"\"  # <- paste your token or leave blank if not required\n",
    "MODEL_NAME = \"mistralai/Mistral-7B-Instruct-v0.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01391170",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "token = os.environ.get(\"HF_TOKEN\") or None\n",
    "weights_dir = Path(\"/content/context/mistral-weights\")\n",
    "model_source = str(weights_dir) if weights_dir.exists() else MODEL_NAME\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_source, token=token, use_fast=True)\n",
    "\n",
    "device_map = \"auto\" if torch.cuda.is_available() else None\n",
    "dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_source,\n",
    "    token=token,\n",
    "    torch_dtype=dtype,\n",
    "    device_map=device_map,\n",
    ")\n",
    "if device_map is None:\n",
    "    model.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "711f3045",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model and tokenizer weights locally (optional)\n",
    "from pathlib import Path\n",
    "\n",
    "save_dir = Path(\"/content/context/mistral-weights\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model.save_pretrained(save_dir)\n",
    "tokenizer.save_pretrained(save_dir)\n",
    "print(f\"Saved to: {save_dir.resolve()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b31d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(user_prompt: str, system_prompt: str = \"You are a helpful assistant.\") -> str:\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt},\n",
    "    ]\n",
    "    if hasattr(tokenizer, \"apply_chat_template\"):\n",
    "        prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    else:\n",
    "        prompt = f\"[SYSTEM] {system_prompt}\\n[USER] {user_prompt}\\n[ASSISTANT]\"\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    output_ids = model.generate(\n",
    "        **inputs,\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    generated = output_ids[0][inputs[\"input_ids\"].shape[1] :]\n",
    "    return tokenizer.decode(generated, skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "273cb0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Explain zero-shot reasoning in simple terms.\"\n",
    "print(generate_response(prompt))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
